{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "PhmkYmjCTuko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from snowballstemmer import TurkishStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "import pandas as pd\n",
        "\n",
        "# Function for text preprocessing\n",
        "def preprocess_text_turkish(text):\n",
        "    text = text.lower()  # Lowercase\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    tokens = word_tokenize(text, language='turkish')  # Tokenize in Turkish\n",
        "    stop_words = set(stopwords.words('turkish'))  # Turkish stopwords\n",
        "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
        "    stemmer = TurkishStemmer()  # Turkish stemmer\n",
        "    tokens = [stemmer.stemWord(word) for word in tokens]  # Stemming\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Load the dataset (adjust the path as needed)\n",
        "data_path = 'Data.xlsx'  # Replace with your file path\n",
        "data = pd.read_excel(data_path, sheet_name='Data')\n",
        "\n",
        "# Preprocess the dataset\n",
        "data['Clean_Comment_Turkish'] = data['Comment'].apply(preprocess_text_turkish)\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(data['Clean_Comment_Turkish'])\n",
        "y = data['Topic']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the SVM classifier\n",
        "svm_classifier = SVC()\n",
        "\n",
        "# Train the SVM classifier\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"SVM Classifier: Accuracy = {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlUyFMYmsq-b",
        "outputId": "a738b461-9409-485b-da17-5f14b455e219"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Classifier: Accuracy = 0.88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming 'data' is your DataFrame with raw text and labels\n",
        "X_raw = data['Comment']  # Replace with your column name\n",
        "y_raw = data['Topic']  # Replace with your column name\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_raw)\n",
        "X_seq = tokenizer.texts_to_sequences(X_raw)\n",
        "X_padded = pad_sequences(X_seq, padding='post', maxlen=50)\n",
        "\n",
        "# Encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y_raw)\n",
        "y_categorical = to_categorical(y_encoded)\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "# Neural Network Model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=5000, output_dim=64, input_length=50),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(y_categorical.shape[1], activation='softmax')  # Use 'sigmoid' for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])  # Use 'binary_crossentropy' for binary\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Neural Network Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVT8kzPCmKPC",
        "outputId": "4f44f698-4c90-4699-8c89-225d7a825fdf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "681/681 [==============================] - 8s 7ms/step - loss: 1.5222 - accuracy: 0.4756\n",
            "Epoch 2/10\n",
            "681/681 [==============================] - 3s 5ms/step - loss: 0.6767 - accuracy: 0.8103\n",
            "Epoch 3/10\n",
            "681/681 [==============================] - 4s 6ms/step - loss: 0.4832 - accuracy: 0.8600\n",
            "Epoch 4/10\n",
            "681/681 [==============================] - 4s 6ms/step - loss: 0.3934 - accuracy: 0.8841\n",
            "Epoch 5/10\n",
            "681/681 [==============================] - 3s 5ms/step - loss: 0.3327 - accuracy: 0.9025\n",
            "Epoch 6/10\n",
            "681/681 [==============================] - 3s 5ms/step - loss: 0.2953 - accuracy: 0.9130\n",
            "Epoch 7/10\n",
            "681/681 [==============================] - 5s 7ms/step - loss: 0.2677 - accuracy: 0.9187\n",
            "Epoch 8/10\n",
            "681/681 [==============================] - 3s 5ms/step - loss: 0.2533 - accuracy: 0.9217\n",
            "Epoch 9/10\n",
            "681/681 [==============================] - 3s 5ms/step - loss: 0.2362 - accuracy: 0.9234\n",
            "Epoch 10/10\n",
            "681/681 [==============================] - 4s 6ms/step - loss: 0.2211 - accuracy: 0.9280\n",
            "Neural Network Accuracy: 0.89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Türkçe için metin ön işleme adımları\n",
        "def preprocess_text_turkish(text):\n",
        "    text = text.lower()  # Küçük harfe dönüştürme\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Noktalama işaretlerini kaldırma\n",
        "    tokens = word_tokenize(text, language='turkish')  # Tokenleme için Türkçe\n",
        "    stop_words = set(stopwords.words('turkish'))  # Türkçe stop-word'leri alma\n",
        "    tokens = [word for word in tokens if word not in stop_words]  # Stop-word'leri kaldırma\n",
        "    stemmer = TurkishStemmer()  # Türkçe kök çıkarma\n",
        "    tokens = [stemmer.stemWord(word) for word in tokens]  # Kök çıkarma\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'Data.xlsx'\n",
        "data = pd.read_excel(file_path, sheet_name='Data')\n",
        "\n",
        "# Preprocess the dataset\n",
        "data['Clean_Comment_Turkish'] = data['Comment'].apply(preprocess_text_turkish)\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(data['Clean_Comment_Turkish'])\n",
        "X_seq = tokenizer.texts_to_sequences(data['Clean_Comment_Turkish'])\n",
        "X_padded = pad_sequences(X_seq, padding='post', maxlen=50)\n",
        "\n",
        "# Encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(data['Topic'])\n",
        "y_categorical = to_categorical(y_encoded)\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "# Neural Network Model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=5000, output_dim=64, input_length=50),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(y_categorical.shape[1], activation='softmax')  # Use 'sigmoid' for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])  # Use 'binary_crossentropy' for binary\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
        "\n",
        "# User input for prediction\n",
        "new_comment = input(\"Enter a new comment: \")\n",
        "\n",
        "# Preprocess the new comment\n",
        "new_comment_clean = preprocess_text_turkish(new_comment)\n",
        "\n",
        "# Tokenize and pad the new comment\n",
        "new_comment_seq = tokenizer.texts_to_sequences([new_comment_clean])\n",
        "new_comment_padded = pad_sequences(new_comment_seq, padding='post', maxlen=50)\n",
        "\n",
        "# Predict with the model\n",
        "probabilities = model.predict(new_comment_padded)[0]\n",
        "\n",
        "# Display probabilities for each category\n",
        "all_categories = label_encoder.classes_\n",
        "for i, category in enumerate(all_categories):\n",
        "    print(f\"{category}: {probabilities[i]*100:.2f}%\")\n",
        "\n",
        "# Find the most likely category\n",
        "most_likely_category = all_categories[np.argmax(probabilities)]\n",
        "print(f\"The probable category of the comment: {most_likely_category}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Mz3MGSJmc-T",
        "outputId": "259f33b2-d0e6-4172-da3f-33434dcff364"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "681/681 [==============================] - 4s 5ms/step - loss: 1.5495 - accuracy: 0.4550\n",
            "Epoch 2/10\n",
            "681/681 [==============================] - 3s 5ms/step - loss: 0.6989 - accuracy: 0.7996\n",
            "Epoch 3/10\n",
            "681/681 [==============================] - 5s 7ms/step - loss: 0.4874 - accuracy: 0.8586\n",
            "Epoch 4/10\n",
            "681/681 [==============================] - 3s 5ms/step - loss: 0.3991 - accuracy: 0.8822\n",
            "Epoch 5/10\n",
            "681/681 [==============================] - 3s 5ms/step - loss: 0.3544 - accuracy: 0.8963\n",
            "Epoch 6/10\n",
            "681/681 [==============================] - 4s 6ms/step - loss: 0.3229 - accuracy: 0.9051\n",
            "Epoch 7/10\n",
            "681/681 [==============================] - 4s 6ms/step - loss: 0.2987 - accuracy: 0.9095\n",
            "Epoch 8/10\n",
            "681/681 [==============================] - 4s 6ms/step - loss: 0.2774 - accuracy: 0.9136\n",
            "Epoch 9/10\n",
            "681/681 [==============================] - 4s 6ms/step - loss: 0.2644 - accuracy: 0.9166\n",
            "Epoch 10/10\n",
            "681/681 [==============================] - 5s 7ms/step - loss: 0.2476 - accuracy: 0.9214\n",
            "Enter a new comment: ürünün sesi çok fazla çıkmıyordu ve ebatı da çok küçültü iade ettim\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "ağırlık: 0.14%\n",
            "hediye: 0.00%\n",
            "kalite: 0.06%\n",
            "kalıp/boy/ölçü: 23.01%\n",
            "kullanımı kolay: 0.01%\n",
            "renk: 15.84%\n",
            "ses: 60.94%\n",
            "çeyiz: 0.00%\n",
            "The probable category of the comment: ses\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import contextlib\n",
        "from openpyxl import load_workbook\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# İlk attığınız kodda tanımlanan modeli ve diğer gerekli nesneleri yükleme\n",
        "# model, tokenizer, label_encoder gibi nesnelerin tanımlandığı varsayılıyor\n",
        "\n",
        "# Input ve Output path'lerini belirleme\n",
        "input_path = 'amazon_reviews.xlsx'\n",
        "output_path = 'amazon_reviews_categorized.xlsx'\n",
        "\n",
        "# Input Excel dosyasını okuma (başlık yoksa)\n",
        "data = pd.read_excel(input_path, header=None)\n",
        "\n",
        "# Yorumların bulunduğu sütunun indeksi (örneğin, ilk sütun için 0)\n",
        "comment_column_index = 0\n",
        "\n",
        "# Yorumları modelle etiketleme\n",
        "def label_comment(comment):\n",
        "    new_comment_clean = preprocess_text_turkish(comment)\n",
        "    new_comment_seq = tokenizer.texts_to_sequences([new_comment_clean])\n",
        "    new_comment_padded = pad_sequences(new_comment_seq, padding='post', maxlen=50)\n",
        "    probabilities = model.predict(new_comment_padded)[0]\n",
        "\n",
        "    # Olasılığı %35'un üstünde olan kategorileri bulma\n",
        "    threshold = 0.35\n",
        "    likely_categories = [label_encoder.classes_[i] for i, prob in enumerate(probabilities) if prob > threshold]\n",
        "\n",
        "    # Kategorilere ait olasılıkları alarak yüzdeye çevirme\n",
        "    category_probabilities = [f\"{prob * 100:.2f}%\" for i, prob in enumerate(probabilities) if label_encoder.classes_[i] in likely_categories]\n",
        "\n",
        "    return likely_categories, category_probabilities\n",
        "\n",
        "# Her yorum için etiket ekleme\n",
        "data['Predicted_Topics'], data['Probabilities'] = zip(*data[comment_column_index].apply(label_comment))\n",
        "\n",
        "# Excel'e yazdırma işlemi\n",
        "with open(os.devnull, 'w') as nullfile:\n",
        "    with contextlib.redirect_stdout(nullfile):\n",
        "        writer = pd.ExcelWriter(output_path, engine='xlsxwriter')\n",
        "        for label in data['Predicted_Topics'].explode().unique():\n",
        "            if isinstance(label, str):  # Check if label is already a string\n",
        "                cleaned_label = label.replace(\"/\", \"_\")  # Replace \"/\" with \"_\"\n",
        "            else:\n",
        "                cleaned_label = str(label)  # Convert non-string types to strings and then replace\n",
        "                cleaned_label = cleaned_label.replace(\"/\", \"_\")  # Replace \"/\" with \"_\"\n",
        "\n",
        "            labeled_data = data[data['Predicted_Topics'].apply(lambda x: label in x)]\n",
        "            labeled_data.to_excel(writer, sheet_name=cleaned_label, index=False, columns=[comment_column_index])\n",
        "\n",
        "        # 'All' kısmında Comments ve Predicted Topics kolonları olsun\n",
        "        all_data = data.explode('Predicted_Topics')[[comment_column_index, 'Predicted_Topics', 'Probabilities']]\n",
        "        all_data.to_excel(writer, sheet_name='All', index=False, header=['Comments', 'Predicted Topics', 'Probabilities'])\n",
        "\n",
        "        # Excel dosyasını kaydetme ve kapatma\n",
        "        writer.save()\n",
        "\n",
        "print(f\"{output_path} dosyası başarıyla oluşturuldu ve kaydedildi.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7GVEhntxqGE",
        "outputId": "b968de65-41cc-4b9b-ff88-4be55c68b0cb"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "amazon_reviews_categorized.xlsx dosyası başarıyla oluşturuldu ve kaydedildi.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-38-7c0342407633>:58: FutureWarning: save is not part of the public API, usage can give unexpected results and will be removed in a future version\n",
            "  writer.save()\n"
          ]
        }
      ]
    }
  ]
}